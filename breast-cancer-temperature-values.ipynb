{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b1df1a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-20T11:20:58.232755Z",
     "iopub.status.busy": "2024-06-20T11:20:58.232336Z",
     "iopub.status.idle": "2024-06-20T11:21:05.573392Z",
     "shell.execute_reply": "2024-06-20T11:21:05.571803Z"
    },
    "papermill": {
     "duration": 7.353263,
     "end_time": "2024-06-20T11:21:05.576934",
     "exception": false,
     "start_time": "2024-06-20T11:20:58.223671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import models,transforms\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9892a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:21:05.590030Z",
     "iopub.status.busy": "2024-06-20T11:21:05.589058Z",
     "iopub.status.idle": "2024-06-20T11:21:05.596781Z",
     "shell.execute_reply": "2024-06-20T11:21:05.595556Z"
    },
    "papermill": {
     "duration": 0.017083,
     "end_time": "2024-06-20T11:21:05.599586",
     "exception": false,
     "start_time": "2024-06-20T11:21:05.582503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read temperature values from a file\n",
    "def read_temperature_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            data = []\n",
    "            for line in lines:\n",
    "                # Clean and split the line\n",
    "                line_data = line.strip().split()\n",
    "                # Convert to float and append\n",
    "                data.extend([float(val) for val in line_data])\n",
    "            return np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873e2ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:21:05.612387Z",
     "iopub.status.busy": "2024-06-20T11:21:05.611367Z",
     "iopub.status.idle": "2024-06-20T11:21:05.619481Z",
     "shell.execute_reply": "2024-06-20T11:21:05.617781Z"
    },
    "papermill": {
     "duration": 0.017558,
     "end_time": "2024-06-20T11:21:05.622469",
     "exception": false,
     "start_time": "2024-06-20T11:21:05.604911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load temperature data and labels from a directory\n",
    "def load_data_from_directory(directory, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if filepath.endswith('.txt'):\n",
    "            temp_values = read_temperature_file(filepath)\n",
    "            if temp_values is not None:\n",
    "                data.append(temp_values)\n",
    "                labels.append(label)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f04456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:21:05.635089Z",
     "iopub.status.busy": "2024-06-20T11:21:05.634647Z",
     "iopub.status.idle": "2024-06-20T11:21:05.640301Z",
     "shell.execute_reply": "2024-06-20T11:21:05.639065Z"
    },
    "papermill": {
     "duration": 0.014975,
     "end_time": "2024-06-20T11:21:05.642793",
     "exception": false,
     "start_time": "2024-06-20T11:21:05.627818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_healthy = '/kaggle/input/breast-cancer-temp-values/Breast_cancer_temp_values/Healthy /Training '\n",
    "testing_healthy = '/kaggle/input/breast-cancer-temp-values/Breast_cancer_temp_values/Healthy /Testing'\n",
    "training_sick = '/kaggle/input/breast-cancer-temp-values/Breast_cancer_temp_values/Sick/Training'\n",
    "testing_sick = '/kaggle/input/breast-cancer-temp-values/Breast_cancer_temp_values/Sick/Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d74a648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:21:05.656135Z",
     "iopub.status.busy": "2024-06-20T11:21:05.654959Z",
     "iopub.status.idle": "2024-06-20T11:23:39.033116Z",
     "shell.execute_reply": "2024-06-20T11:23:39.031848Z"
    },
    "papermill": {
     "duration": 153.387939,
     "end_time": "2024-06-20T11:23:39.036205",
     "exception": false,
     "start_time": "2024-06-20T11:21:05.648266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading /kaggle/input/breast-cancer-temp-values/Breast_cancer_temp_values/Sick/Training/PAC_36_ESTATICO.txt: could not convert string to float: '31,1'\n",
      "Error reading /kaggle/input/breast-cancer-temp-values/Breast_cancer_temp_values/Sick/Training/PAC_35_ESTATICO.txt: could not convert string to float: '31,78'\n"
     ]
    }
   ],
   "source": [
    "# Load data and labels\n",
    "train_data_healthy, train_labels_healthy = load_data_from_directory(training_healthy, 0)\n",
    "train_data_sick, train_labels_sick = load_data_from_directory(training_sick, 1)\n",
    "test_data_healthy, test_labels_healthy = load_data_from_directory(testing_healthy, 0)\n",
    "test_data_sick, test_labels_sick = load_data_from_directory(testing_sick, 1)\n",
    "\n",
    "# Combine healthy and sick data\n",
    "train_data = train_data_healthy + train_data_sick\n",
    "train_labels = train_labels_healthy + train_labels_sick\n",
    "test_data = test_data_healthy + test_data_sick\n",
    "test_labels = test_labels_healthy + test_labels_sick\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb2c88e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:23:39.049593Z",
     "iopub.status.busy": "2024-06-20T11:23:39.049015Z",
     "iopub.status.idle": "2024-06-20T11:23:40.044450Z",
     "shell.execute_reply": "2024-06-20T11:23:40.042352Z"
    },
    "papermill": {
     "duration": 1.005488,
     "end_time": "2024-06-20T11:23:40.047547",
     "exception": false,
     "start_time": "2024-06-20T11:23:39.042059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (756, 307200)\n",
      "Validation data shape: (189, 307200)\n",
      "Test data shape: (180, 307200)\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into training and validation sets (80-20 split)\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbef88d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:23:40.061108Z",
     "iopub.status.busy": "2024-06-20T11:23:40.059848Z",
     "iopub.status.idle": "2024-06-20T11:23:40.067418Z",
     "shell.execute_reply": "2024-06-20T11:23:40.066359Z"
    },
    "papermill": {
     "duration": 0.016979,
     "end_time": "2024-06-20T11:23:40.070029",
     "exception": false,
     "start_time": "2024-06-20T11:23:40.053050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a PyTorch dataset\n",
    "class TemperatureDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03466eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:23:40.082431Z",
     "iopub.status.busy": "2024-06-20T11:23:40.082033Z",
     "iopub.status.idle": "2024-06-20T11:23:40.742971Z",
     "shell.execute_reply": "2024-06-20T11:23:40.741952Z"
    },
    "papermill": {
     "duration": 0.670389,
     "end_time": "2024-06-20T11:23:40.745747",
     "exception": false,
     "start_time": "2024-06-20T11:23:40.075358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TemperatureDataset(train_data, train_labels)\n",
    "val_dataset = TemperatureDataset(val_data, val_labels)\n",
    "test_dataset = TemperatureDataset(test_data, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cacc5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:23:40.759214Z",
     "iopub.status.busy": "2024-06-20T11:23:40.758779Z",
     "iopub.status.idle": "2024-06-20T11:23:41.659714Z",
     "shell.execute_reply": "2024-06-20T11:23:41.658435Z"
    },
    "papermill": {
     "duration": 0.910866,
     "end_time": "2024-06-20T11:23:41.662664",
     "exception": false,
     "start_time": "2024-06-20T11:23:40.751798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplexNN(\n",
       "  (fc1): Linear(in_features=307200, out_features=256, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop3): Dropout(p=0.5, inplace=False)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a more complex neural network\n",
    "class ComplexNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ComplexNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.drop2(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.drop3(x)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = train_data.shape[1]\n",
    "model = ComplexNN(input_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92b856b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:23:41.676005Z",
     "iopub.status.busy": "2024-06-20T11:23:41.675568Z",
     "iopub.status.idle": "2024-06-20T11:29:57.787534Z",
     "shell.execute_reply": "2024-06-20T11:29:57.786059Z"
    },
    "papermill": {
     "duration": 376.129158,
     "end_time": "2024-06-20T11:29:57.797538",
     "exception": false,
     "start_time": "2024-06-20T11:23:41.668380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.5402, Val Loss: 1.6971, Val Accuracy: 66.14%\n",
      "Epoch 2/20, Train Loss: 0.3538, Val Loss: 0.2416, Val Accuracy: 93.12%\n",
      "Epoch 3/20, Train Loss: 0.2597, Val Loss: 0.2670, Val Accuracy: 91.53%\n",
      "Epoch 4/20, Train Loss: 0.1656, Val Loss: 0.3565, Val Accuracy: 84.13%\n",
      "Epoch 5/20, Train Loss: 0.1361, Val Loss: 0.4644, Val Accuracy: 76.19%\n",
      "Epoch 6/20, Train Loss: 0.0897, Val Loss: 0.2082, Val Accuracy: 92.59%\n",
      "Epoch 7/20, Train Loss: 0.0781, Val Loss: 0.0680, Val Accuracy: 99.47%\n",
      "Epoch 8/20, Train Loss: 0.0710, Val Loss: 0.0563, Val Accuracy: 99.47%\n",
      "Epoch 9/20, Train Loss: 0.0405, Val Loss: 2.3819, Val Accuracy: 37.04%\n",
      "Epoch 10/20, Train Loss: 0.0356, Val Loss: 0.6910, Val Accuracy: 66.67%\n",
      "Epoch 11/20, Train Loss: 0.0375, Val Loss: 0.0209, Val Accuracy: 100.00%\n",
      "Epoch 12/20, Train Loss: 0.0290, Val Loss: 0.0125, Val Accuracy: 100.00%\n",
      "Epoch 13/20, Train Loss: 0.0247, Val Loss: 0.0110, Val Accuracy: 100.00%\n",
      "Epoch 14/20, Train Loss: 0.0296, Val Loss: 0.0264, Val Accuracy: 100.00%\n",
      "Epoch 15/20, Train Loss: 0.0208, Val Loss: 0.0304, Val Accuracy: 98.94%\n",
      "Epoch 16/20, Train Loss: 0.0193, Val Loss: 0.0106, Val Accuracy: 100.00%\n",
      "Epoch 17/20, Train Loss: 0.0145, Val Loss: 0.0086, Val Accuracy: 100.00%\n",
      "Epoch 18/20, Train Loss: 0.0106, Val Loss: 0.0043, Val Accuracy: 100.00%\n",
      "Epoch 19/20, Train Loss: 0.0160, Val Loss: 0.0068, Val Accuracy: 100.00%\n",
      "Epoch 20/20, Train Loss: 0.0089, Val Loss: 0.0077, Val Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "num_epochs = 20\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, labels in train_dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_dataloader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9807b3c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T11:29:57.814656Z",
     "iopub.status.busy": "2024-06-20T11:29:57.814248Z",
     "iopub.status.idle": "2024-06-20T11:29:58.629314Z",
     "shell.execute_reply": "2024-06-20T11:29:58.627696Z"
    },
    "papermill": {
     "duration": 0.827198,
     "end_time": "2024-06-20T11:29:58.632227",
     "exception": false,
     "start_time": "2024-06-20T11:29:57.805029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 97.78%\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        outputs = model(data).squeeze()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on test set: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381e9da",
   "metadata": {
    "papermill": {
     "duration": 0.007357,
     "end_time": "2024-06-20T11:29:58.648218",
     "exception": false,
     "start_time": "2024-06-20T11:29:58.640861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5205441,
     "sourceId": 8682858,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 546.949496,
   "end_time": "2024-06-20T11:30:01.562165",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-20T11:20:54.612669",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
